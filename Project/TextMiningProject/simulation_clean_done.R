library(MCMCpack)
# Generating the data
wordtypes <- read.table(file="~/Dropbox/Academic notes/5 A/STAT 444/Project/listingwords.txt")
# Dont even really need to define the topics like this, however, these words are essentially the "most frequently occuring"
# or highest pdf valued words in each topic.
topic1 <- c("kitchen","basement","entrance","beautiful","detached","hardwood","stunning","bedroom","roof",
            "4","location","painted","prime","backsplash","bright","ensuite","excellent","famous","granite",
            "interlock","laundry","maintained","master","sidewalk","upgraded")
topic2 <- c("yard","apartment","backyard","bathroom","beauty","bedrooms","brick","ceramic","community","deck",
            "demand","drive","family","finished","fireplace","freshly","front",
            "furnace","garage","high","home","house","landscaped","layout","new")
topic3 <- c("oak","renovated","school","schools","walk","windows","2015","3","absolutely","amazing","amenities",
            "appliances","area","bath","bathrooms","baths","bottom","breakfast","close","closet","concept",
            "countertop","crown","desirable","dishwasher")
topic4 <- c("distance","enclosed","extra","features","fenced","fridges","fully","gleaming","gorgeous","grand",
            "hallway","huge","immaculate","in","lake","landscaping","large","lighting","lights","lot","lots","luxurious",
            "main","newer","open")
topic5 <- c("park","parking","parks","porch","pot","potential","practical","private","public","restaurants",
            "room","shed","shopping","sought","spaces","spacious","tank","transit","updated","virtual",
            "walking","walkway","washrooms","water","well")
topic6 <- c("wide","1970","fancy","mahogany","!","modern","tranquil",".","?","quiet","young","youthful",
            "upcoming","cinema","grocery","gym","trail","cozy","5","2011","loud","1966","historic","winter","available")
# Random variables
set.seed(1)
# Creating random parameters for each poisson variable used to create 
# listing lengths
lambdas <- 1:10
for(i in 1:10){ lambdas[i] <- rgamma(1,18)}
# For each document, generate a random length/ number of unique word types
m <- 1:10
for(i in 1:10){ m[i] <- rpois(1,lambdas[i])}
# For each topic, making a distribution for each word
P <- matrix(0, ncol = 150,nrow = 6)
for(k in 1:6){ P[k,] <- rdirichlet(1,rep(0.05,150))}
# Determining the topic distribution for each documents i
Zeta <- matrix(0, ncol = 6,nrow = 10)
for(i in 1:10) { 
        Zeta[i,] <- rdirichlet(1,rep(0.1,6))
}
# Simulating the topic for each of the mi word types in document i, position m
k <-   matrix(0, ncol = 30,nrow = 10)
for(i in 1:10) { 
        k[i,] <- rmultinom(1,6,Zeta[i,]) 
        k[i,k[i,] == 0] <- 1
        k[i,k[i,] == 4] <- 2
        k[i,k[i,] == 6] <- 3
        k[i,k[i,] == 5] <- 4
        k[i,k[i,] == 2] <- 5
        # Adding zeros instead of topics for empty spots in documents
        n <- m[i]
        if(n < ncol(k)){
                # Extra spaces generates
                fill <- rep(0,ncol(k) - n)
                k[i,(n+1):ncol(k)] <- fill        
        }
}
# indices for w given the topic for each spot m in doc i
w.given.k.index <- k
# list to hold the generated actual word types for each spot m in doc i
w.given.k <- list()
# Pulling word types based on the distribution of mth word, ith topic
for(i in 1:10){
        for(j in 1:30){
                if(w.given.k.index[i,j] != 0){
                        w.given.k.index[i,j] <- which(rmultinom(1,1,
                                                                P[w.given.k.index[i,j],]) != 0)
                } else {
                        w.given.k.index[i,j] <- 132
                }
        }
        
        w.given.k[[i]] <- wordtypes[w.given.k.index[i,],1] 
}
w.given.k <- data.frame(matrix(unlist(w.given.k), nrow=10, byrow=T))
addemptyString <- function(x){
        if(is.factor(x)) return(factor(x, levels=c(levels(x), "")))
        return(x)
}
w.given.k <- as.data.frame(lapply(w.given.k, addemptyString))
w.given.k[w.given.k == "."] <- ""
# Vocabulary of M = 150 word types
# Words in n = 10 documents generated by a mixture of K = 6 topics each defined by 
# probability distributions P1..P6 over the 150 word types 

# W
words <-scan("~/Dropbox/Academic notes/5 A/STAT 444/Project/listingblob.txt",
             what="char", sep="\n")
# Converting the read in tect listings to lower case
blobi <-tolower(words)
# Splitting the text into individual word types
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
# Changing a list of word types into an R vector data type
blob.words.vector<-unlist(blob.list)
sort(table(blob.words.vector),decreasing = TRUE)
# Vector of all the unique word types occurring in listings
allwords <- unique(blob.words.vector)
# Creating a matrix W to hold the word type counts
W <- matrix(0,ncol=length(allwords),nrow=10)
# Assigning the counts of the word types in W
for(i in 1:10){
        for(j in 1:length(names(blobs[[i]]))){
                W[i,which((names(blobs[[i]])[j] == allwords) ==1)] <- blobs[[i]][[j]]
                }
        }
}
# Reading in the randomly generated listings from a text file
blob <-scan("~/Dropbox/Academic notes/5 A/STAT 444/Project/sample-listings.txt",
                          what="char", sep="\n")
# Creating a list to hold the word type counts for each listing
blobs <- list()
for(i in 1:10){
blobi <-tolower(blob[i])
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
blob.words.vector<-unlist(blob.list)
blob.freq.list<-table(blob.words.vector)
print(length(blob.freq.list))
blobs[[i]] <- blob.freq.list
}
# Creating a list to hold the bigrams occurring in each listing
Big <- list()
# Looping through each listing and creating a list of the bigrams occuring in each one
for(i in 1:10){
        blobi <-tolower(blob[i])
        blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
        blob.words.vector <- unlist(blob.list)
        bigrams <- rep(0,length(blob.words.vector) -1)
        for(i in 1:length(blob.words.vector) -1){
                bigrams[i] <- paste(blob.words.vector[i],blob.words.vector[i+1] , sep = ",")
        }
        Big[[i]] <- bigrams
}
# Creating a vector with each of the bigrams that occurs in the whole corpus
B2 <- unlist(Big)
# Getting a subset of the unique bigrams occuring in the corpus
B3 <- unique(B2)
# Viewing the bigrams occurring more than once
table(B2)[table(B2) > 1]
# Creating a dataframe to hold the bigrams that appeared and their respective counts - initializing all to 1 occurence
B.final <- data.frame(B3,count=rep(1,length(B3)))
table(B2)[table(B2) > 1]
# Adjusting the counts of bigrams that occurred more than once [hard-coded]
B.final[B3=="4,bedroom",2] <- 4
B.final[B3=="entrance,kitchen",2] <- 3
B.final[B3=="basement,detached",2] <- 2
B.final[B3=="hardwood,basement",2] <- 2
B.final[B3=="kitchen,basement",2] <- 2
B.final[B3=="maintained,detached",2] <- 2
B.final[B3=="sidewalk,interlock",2] <- 2
B.final[B3=="upgraded,ceramic",2] <- 2
# Data frame of all the bigrams and their rspective counts: data.frame(string int)
B.final <- B.final[with(B.final, order(B3)), ]

# Getting a full listing of all words in the corpus, disregarding different documents
words <-scan("~/Dropbox/Academic notes/5 A/STAT 444/Project/listingblob.txt",
            what="char", sep="\n")
# Changing the corpus to lower case
blobi <-tolower(words)
# Splitting the corpus into individual word tokens
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
# Changing the list of word tokens into a R vector data type
blob.words.vector<-unlist(blob.list)
# Extracting only the unique word tokens
blob.words.vector <- unique(blob.words.vector)
# Creating a data frame with all the possible bigrams for comparison with the list
# of bigrams that occurred in the text
for(i in 1:length(blob.words.vector)){
        for(j in 1:length(blob.words.vector)){
                if(j == 1){
                        column <- c()
                        column <- rbind(column,
                                        paste(blob.words.vector[i],
                                              blob.words.vector[j],
                                              sep = ","))
                } else {
                        column <- rbind(column,
                                        paste(blob.words.vector[i],
                                                     blob.words.vector[j],
                                                     sep = ","))
                }
        }
        if(i == 1){ 
                Bi <- data.frame(column)
        }
        else{ Bi <- data.frame(Bi,column)}
}
# Creating a matrix of same dimension as B to hold the counts of bigrams as the list calculated is
# compared against the data frame with all the possibilities
Bi2 <- matrix(0, ncol = 126, nrow = 126)
# Assigning each bigram count to its respective column in B
for(i in 1:nrow(Bi)){
        for(j in 1:ncol(Bi)){
                #B.final[,1] <- factor(B.final[,1], levels=levels(Bi[,j))
                if(is.na(match(Bi[i,j],B.final[,1]))){
                        Bi2[i,j] = 0
                } else{
                        Bi2[i,j] = B.final[match(Bi[i,j],B.final[,1]),2]
                }
        }
}
B <- Bi2
# B is the bigram matrix
# Coefficients for the response/s weighted sum
Betas <- c(top1=1500000,top2=900000,top3=750000,top4=500000,top5=150000,top6=0)
# Response vector
# Calculating mean prices for each listing based on the distribution of topics in the document
means <- Zeta%*%Betas
# Creating a data frame to hold the simulated listing prices
y <- data.frame()
# Creating the listing prices with a  normal random variable
set.seed(1)
for(i in 1:10){
        y <- round(rbind(y,rnorm(1,mean =means[i],sd=75000)))
        names(y) <- "Price"
}

# Result simulated data
W ; B ; y
data <- data.frame(Zeta,Y=y)
summary(lm(Price~.,data=data))
# Using 5 principal components
kW <- 5 ; KB <- 5
# W.pca <- prcomp(W,scale=TRUE,center=TRUE)
# W.prin.comps <- W.pca$x[,1:kW] %*% t(W.pca$rotation[,1:kW])
# #and add the center (and re-scale) back to data
# if(W.pca$scale != FALSE){
#         W.prin.comps <- scale(W.prin.comps, center = FALSE , scale=1/W.pca$scale)
# }
# if(W.pca$center != FALSE){
#         W.prin.comps <- scale(W.prin.comps, center = -1 * W.pca$center, scale=FALSE)
# }
# dim(W.prin.comps); dim(W)
# Adjusted R square of .98
# 4,bedroom        4
# 2	entrance,kitchen	3
# 3	basement,detached	2
# 4	entrance,basement	2
# 5	hardwood,basement	2
# 6	kitchen,basement	2
# 7	maintained,detached	2
# 8	sidewalk,interlock	2
# 9	upgraded,ceramic	2
