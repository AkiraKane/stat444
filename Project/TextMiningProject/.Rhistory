topic6 <- c("wide","1970","fancy","mahogany","!","modern","tranquil",".","?","quiet","young","youthful",
"upcoming","cinema","grocery","gym","trail","cozy","5","2011","loud","1966","historic","winter","available")
# Random variables
set.seed(1)
# Creating random parameters for each poisson variable used to create
# listing lengths
lambdas <- 1:10
for(i in 1:10){ lambdas[i] <- rgamma(1,18)}
# For each document, generate a random length/ number of unique word types
m <- 1:10
for(i in 1:10){ m[i] <- rpois(1,lambdas[i])}
# For each topic, making a distribution for each word
P <- matrix(0, ncol = 150,nrow = 6)
for(k in 1:6){ P[k,] <- rdirichlet(1,rep(0.05,150))}
# Determining the topic distribution for each documents i
Zeta <- matrix(0, ncol = 6,nrow = 10)
for(i in 1:10) {
Zeta[i,] <- rdirichlet(1,rep(0.1,6))
}
# Simulating the topic for each of the mi word types in document i, position m
k <-   matrix(0, ncol = 30,nrow = 10)
for(i in 1:10) {
k[i,] <- rmultinom(1,6,Zeta[i,])
k[i,k[i,] == 0] <- 1
k[i,k[i,] == 4] <- 2
k[i,k[i,] == 6] <- 3
k[i,k[i,] == 5] <- 4
k[i,k[i,] == 2] <- 5
# Adding zeros instead of topics for empty spots in documents
n <- m[i]
if(n < ncol(k)){
# Extra spaces generates
fill <- rep(0,ncol(k) - n)
k[i,(n+1):ncol(k)] <- fill
}
}
# indices for w given the topic for each spot m in doc i
w.given.k.index <- k
# list to hold the generated actual word types for each spot m in doc i
w.given.k <- list()
# Pulling word types based on the distribution of mth word, ith topic
for(i in 1:10){
for(j in 1:30){
if(w.given.k.index[i,j] != 0){
w.given.k.index[i,j] <- which(rmultinom(1,1,
P[w.given.k.index[i,j],]) != 0)
} else {
w.given.k.index[i,j] <- 132
}
}
w.given.k[[i]] <- wordtypes[w.given.k.index[i,],1]
}
w.given.k <- data.frame(matrix(unlist(w.given.k), nrow=10, byrow=T))
addemptyString <- function(x){
if(is.factor(x)) return(factor(x, levels=c(levels(x), "")))
return(x)
}
w.given.k <- as.data.frame(lapply(w.given.k, addemptyString))
w.given.k[w.given.k == "."] <- ""
# Vocabulary of M = 150 word types
# Words in n = 10 documents generated by a mixture of K = 6 topics each defined by
# probability distributions P1..P6 over the 150 word types
# W
words <-scan("~/Dropbox/Academic notes/5 A/STAT 444/Project/listingblob.txt",
what="char", sep="\n")
# Converting the read in tect listings to lower case
blobi <-tolower(words)
# Splitting the text into individual word types
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
# Changing a list of word types into an R vector data type
blob.words.vector<-unlist(blob.list)
sort(table(blob.words.vector),decreasing = TRUE)
# Vector of all the unique word types occurring in listings
allwords <- unique(blob.words.vector)
# Creating a matrix W to hold the word type counts
W <- matrix(0,ncol=length(allwords),nrow=10)
# Assigning the counts of the word types in W
for(i in 1:10){
for(j in 1:length(names(blobs[[i]]))){
W[i,which((names(blobs[[i]])[j] == allwords) ==1)] <- blobs[[i]][[j]]
}
}
}
# Reading in the randomly generated listings from a text file
blob <-scan("~/Dropbox/Academic notes/5 A/STAT 444/Project/sample-listings.txt",
what="char", sep="\n")
# Creating a list to hold the word type counts for each listing
blobs <- list()
for(i in 1:10){
blobi <-tolower(blob[i])
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
blob.words.vector<-unlist(blob.list)
blob.freq.list<-table(blob.words.vector)
print(length(blob.freq.list))
blobs[[i]] <- blob.freq.list
}
# Creating a list to hold the bigrams occurring in each listing
Big <- list()
# Looping through each listing and creating a list of the bigrams occuring in each one
for(i in 1:10){
blobi <-tolower(blob[i])
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
blob.words.vector <- unlist(blob.list)
bigrams <- rep(0,length(blob.words.vector) -1)
for(i in 1:length(blob.words.vector) -1){
bigrams[i] <- paste(blob.words.vector[i],blob.words.vector[i+1] , sep = ",")
}
Big[[i]] <- bigrams
}
# Creating a vector with each of the bigrams that occurs in the whole corpus
B2 <- unlist(Big)
# Getting a subset of the unique bigrams occuring in the corpus
B3 <- unique(B2)
# Viewing the bigrams occurring more than once
table(B2)[table(B2) > 1]
# Creating a dataframe to hold the bigrams that appeared and their respective counts - initializing all to 1 occurence
B.final <- data.frame(B3,count=rep(1,length(B3)))
table(B2)[table(B2) > 1]
# Adjusting the counts of bigrams that occurred more than once [hard-coded]
B.final[B3=="4,bedroom",2] <- 4
B.final[B3=="entrance,kitchen",2] <- 3
B.final[B3=="basement,detached",2] <- 2
B.final[B3=="hardwood,basement",2] <- 2
B.final[B3=="kitchen,basement",2] <- 2
B.final[B3=="maintained,detached",2] <- 2
B.final[B3=="sidewalk,interlock",2] <- 2
B.final[B3=="upgraded,ceramic",2] <- 2
# Data frame of all the bigrams and their rspective counts: data.frame(string int)
B.final <- B.final[with(B.final, order(B3)), ]
# Getting a full listing of all words in the corpus, disregarding different documents
words <-scan("~/Dropbox/Academic notes/5 A/STAT 444/Project/listingblob.txt",
what="char", sep="\n")
# Changing the corpus to lower case
blobi <-tolower(words)
# Splitting the corpus into individual word tokens
blob.list<-strsplit(blobi, "\\W+", perl=TRUE)
# Changing the list of word tokens into a R vector data type
blob.words.vector<-unlist(blob.list)
# Extracting only the unique word tokens
blob.words.vector <- unique(blob.words.vector)
# Creating a data frame with all the possible bigrams for comparison with the list
# of bigrams that occurred in the text
for(i in 1:length(blob.words.vector)){
for(j in 1:length(blob.words.vector)){
if(j == 1){
column <- c()
column <- rbind(column,
paste(blob.words.vector[i],
blob.words.vector[j],
sep = ","))
} else {
column <- rbind(column,
paste(blob.words.vector[i],
blob.words.vector[j],
sep = ","))
}
}
if(i == 1){
Bi <- data.frame(column)
}
else{ Bi <- data.frame(Bi,column)}
}
# Creating a matrix of same dimension as B to hold the counts of bigrams as the list calculated is
# compared against the data frame with all the possibilities
Bi2 <- matrix(0, ncol = 126, nrow = 126)
# Assigning each bigram count to its respective column in B
for(i in 1:nrow(Bi)){
for(j in 1:ncol(Bi)){
#B.final[,1] <- factor(B.final[,1], levels=levels(Bi[,j))
if(is.na(match(Bi[i,j],B.final[,1]))){
Bi2[i,j] = 0
} else{
Bi2[i,j] = B.final[match(Bi[i,j],B.final[,1]),2]
}
}
}
B <- Bi2
# B is the bigram matrix
# Coefficients for the response/s weighted sum
Betas <- c(top1=1500000,top2=900000,top3=750000,top4=500000,top5=150000,top6=0)
# Response vector
# Calculating mean prices for each listing based on the distribution of topics in the document
means <- Zeta%*%Betas
# Creating a data frame to hold the simulated listing prices
y <- data.frame()
# Creating the listing prices with a  normal random variable
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=0.5)))
names(y) <- "Price"
}
# Result simulated data
W ; B ; y
W
B
Zeta
summary(lm(y~Zeta))
y
chr(y)
y
y <- as.data.frame(y)
summary(lm(y~Zeta))
y
data <- data.frame(Zeta,Y=y)
summary(lm(Y~.,data=data))
data
is.na(data)
summary(lm(Price~.,data=data))
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=sqrt(150000))))
names(y) <- "Price"
}
data <- data.frame(Zeta,Y=y)
summary(lm(Price~.,data=data))
y
y <- data.frame()
# Creating the listing prices with a  normal random variable
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=sqrt(150000))))
names(y) <- "Price"
}
y
y <- data.frame()
# Creating the listing prices with a  normal random variable
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=sqrt(1500000))))
names(y) <- "Price"
}
y
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=75000)))
names(y) <- "Price"
}
y
y <- data.frame()
# Creating the listing prices with a  normal random variable
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=75000)))
names(y) <- "Price"
}
y
set.seed(1)
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=75000)))
names(y) <- "Price"
}
y
y <- data.frame()
# Creating the listing prices with a  normal random variable
set.seed(1)
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=75000)))
names(y) <- "Price"
}
y
data <- data.frame(Zeta,Y=y)
summary(lm(Price~.,data=data))
?svd
svd(W)
library(caret)
preProcess(W,pcaComp = 5)
preProcess(W,method="pca",pcaComp = 5)
library(stats)
princomp(~.,x=W)
prcomp(~.,x=W,scale.=TRUE)
prcomp(~.,x=W,scale=TRUE)
prcomp(W,scale=TRUE)
W
prcomp(W,scale=TRUE)
?prcomp
svd(W)
prcomp(W,scale=TRUE)$rotation
prcomp(W,scale=TRUE)$x
svd(W)
prcomp(W,scale=TRUE,center=TRUE)$x
kW <- 5 ; KB <- 5
W.pca <- prcomp(W,scale=TRUE,center=TRUE)
W.prin.comps <- W.pca$x[,1:kW] %*% t(W.pca$rotation[,1:kW])
#and add the center (and re-scale) back to data
if(W.pca$scale != FALSE){
W.prin.comps <- scale(W.prin.comps, center = FALSE , scale=1/W.pca$scale)
}
if(W.pca$center != FALSE){
W.prin.comps <- scale(W.prin.comps, center = -1 * W.pca$center, scale=FALSE)
}
W.pca$scale
W.pca <- prcomp(W,scale=FALSE,center=TRUE)
W.pca$scale
W.pca <- prcomp(W,scale=TRUE,center=TRUE)
W.prin.comps <- W.pca$x[,1:kW] %*% t(W.pca$rotation[,1:kW])
#and add the center (and re-scale) back to data
if(W.pca$scale != FALSE){
W.prin.comps <- scale(W.prin.comps, center = FALSE , scale=1/W.pca$scale)
}
if(W.pca$center != FALSE){
W.prin.comps <- scale(W.prin.comps, center = -1 * W.pca$center, scale=FALSE)
}
dim(W.prin.comps); dim(W)
W.prin.comps <- W.pca$x[,1:kW] %*% t(W.pca$rotation[,1:kW])
dim(W.prin.comps); dim(W)
W.pca$x[,1:kW]
t(W.pca$rotation[,1:kW])
W.regressors <- svd(W)$u[1:kW]
svd(W)$u
UW <- svd(W)$u[1:kW]
UB <- svd(B)$u[1:kB]
VB <- svd(B)$v[1:kB]
kW <- 5 ; kB <- 5
UW <- svd(W)$u[1:kW]
UB <- svd(B)$u[1:kB]
VB <- svd(B)$v[1:kB]
VB
svd(B)$v
dim(svd(B)$v)
UW
UW <- svd(W)$u[,1:kW]
UB <- svd(B)$u[,1:kB]
VB <- svd(B)$v[,1:kB]
UW
UB
VB
UB
diag(W)
dim(W)
for(i in 1:10){ avg.w[i,i] <- 1/sum(W[i,]^2)}
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sum(W[i,]^2)}
diag(avg.w)
matrix(data.frame(UB,VB))
matrix(UB,VB))
matrix(UB,VB)
data.frame(UB,VB)
dim(data.frame(UB,VB))
C <- avg.w%*%W%*%data.frame(UB,VB)
dim(avg.w)
dim(W)
C <- diag(avg.w)%*%W%*%data.frame(UB,VB)
dim(diag(avg.w))
diag(avg.w)
diag(avg.w)%*%W
dim(diag(avg.w)%*%W)
dim(data.frame(UB,VB))
as.matrix(data.frame(UB,VB))
C <- diag(avg.w)%*%W%*%as.matrix(data.frame(UB,VB))
C
dim(diag(avg.w)%*%W)
dim(data.frame(UB,VB))
dim(diag(avg.w)%*%W)
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sqrt(sum(W[i,]^2))}
# Correlation matrix C
C <- diag(avg.w)%*%W%*%as.matrix(data.frame(UB,VB))
C
diag(avg.w)*W
diag(avg.w)*W[1,]
(diag(avg.w)*W)[1,]
(diag(avg.w)[1]*W[1,])
(diag(avg.w)*W)[2,]
(diag(avg.w)[2]*W[2,])
C <- diag(avg.w)*W%*%as.matrix(data.frame(UB,VB))
C
# Explanatory variates
design <- data.frame(UW,C)
# Regression model
text.mining.mod <- lm(y~design)
Y <- data.frame(y)
text.mining.mod <- lm(Y~design)
design <- data.frame(UW,C,y)
# Regression model
text.mining.mod <- lm(Price~.,data=design)
summary(text.mining.mod)
design
Betas <- c(top1=1.5,top2=0.9,top3=.75,top4=.5,top5=.15,top6=0)
means <- Zeta%*%Betas
y <- data.frame()
# Creating the listing prices with a  normal random variable
set.seed(1)
for(i in 1:10){
y <- round(rbind(y,rnorm(1,mean =means[i],sd=75000)))
names(y) <- "Price"
}
Betas <- c(top1=1.5,top2=0.9,top3=.75,top4=.5,top5=.15,top6=0)
# Response vector
# Calculating mean prices for each listing based on the distribution of topics in the document
means <- Zeta%*%Betas
# Creating a data frame to hold the simulated listing prices
y <- data.frame()
# Creating the listing prices with a  normal random variable
set.seed(1)
for(i in 1:10){
y <- rbind(y,rnorm(1,mean =means[i],sd=0.5))
Betas <- c(top1=1.5,top2=0.9,top3=.75,top4=.5,top5=.15,top6=0)
# Response vector
# Calculating mean prices for each listing based on the distribution of topics in the document
means <- Zeta%*%Betas
# Creating a data frame to hold the simulated listing prices
y <- data.frame()
# Creating the listing prices with a  normal random variable
set.seed(1)
for(i in 1:10){
y <- rbind(y,rnorm(1,mean =means[i],sd=0.075))
names(y) <- "Price"
}
# Result simulated data
W ; B ; y
data <- data.frame(Zeta,Y=y)
summary(lm(Price~.,data=data))
# Adjusted R square of .98
# Using 5 principal components
kW <- 5 ; kB <- 5
# Principal components of W
UW <- svd(W)$u[,1:kW]
# Principal components of B
UB <- svd(B)$u[,1:kB]
VB <- svd(B)$v[,1:kB]
# creating diag wi^-1
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sqrt(sum(W[i,]^2))}
# Correlation matrix C
C <- diag(avg.w)*W%*%as.matrix(data.frame(UB,VB))
# Explanatory variates
design <- data.frame(UW,C,y)
# Regression model
text.mining.mod <- lm(Price~.,data=design)
summary(text.mining.mod)
C
UW
svd(W)$u
kW <- 8 ; kB <- 8
# Principal components of W
UW <- svd(W)$u[,1:kW]
# Principal components of B
UB <- svd(B)$u[,1:kB]
VB <- svd(B)$v[,1:kB]
# creating diag wi^-1
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sqrt(sum(W[i,]^2))}
# Correlation matrix C
C <- diag(avg.w)*W%*%as.matrix(data.frame(UB,VB))
# Explanatory variates
design <- data.frame(UW,C,y)
# Regression model
text.mining.mod <- lm(Price~.,data=design)
summary(text.mining.mod)
kW <- 3 ; kB <- 3
# Principal components of W
UW <- svd(W)$u[,1:kW]
# Principal components of B
UB <- svd(B)$u[,1:kB]
VB <- svd(B)$v[,1:kB]
# creating diag wi^-1
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sqrt(sum(W[i,]^2))}
# Correlation matrix C
C <- diag(avg.w)*W%*%as.matrix(data.frame(UB,VB))
# Explanatory variates
design <- data.frame(UW,C,y)
# Regression model
text.mining.mod <- lm(Price~.,data=design)
summary(text.mining.mod)
?norm
sqrt(sum(W[1,]^2))
norm(W[1,])
design
summary(text.mining.mod)
plot(text.mining.mod)
for(i in 1:10){
for(j in 1:length(names(blobs[[i]]))){
W[i,which((names(blobs[[i]])[j] == allwords) ==1)] <- blobs[[i]][[j]]
}
}
}
summary(text.mining.mod)
kW <- 1 ; kB <- 1
# Principal components of W
UW <- svd(W)$u[,1:kW]
# Principal components of B
UB <- svd(B)$u[,1:kB]
VB <- svd(B)$v[,1:kB]
# creating diag wi^-1
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sqrt(sum(W[i,]^2))}
# Correlation matrix C
C <- diag(avg.w)*W%*%as.matrix(data.frame(UB,VB))
# Explanatory variates
design <- data.frame(UW,C,y)
# Regression model
text.mining.mod <- lm(Price~.,data=design)
summary(text.mining.mod)
kW <- 2 ; kB <- 2
# Principal components of W
UW <- svd(W)$u[,1:kW]
# Principal components of B
UB <- svd(B)$u[,1:kB]
VB <- svd(B)$v[,1:kB]
# creating diag wi^-1
avg.w <- matrix(0, nrow=10,ncol=126)
for(i in 1:10){ avg.w[i,i] <- 1/sqrt(sum(W[i,]^2))}
# Correlation matrix C
C <- diag(avg.w)*W%*%as.matrix(data.frame(UB,VB))
# Explanatory variates
design <- data.frame(UW,C,y)
# Regression model
text.mining.mod <- lm(Price~.,data=design)
summary(text.mining.mod)
summary(text.mining.mod)
plot(text.mining.mod)
source('~/Dropbox/Academic notes/5 A/STAT 444/Project/TextMiningProject/simulation_final.R')
summary(text.mining.mod)
Rsq(text.mining.mod)
str(text.mining.mod)
summary(text.mining.mod)
summary(text.mining.mod)$r.squared
